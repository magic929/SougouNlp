{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.contrib.keras as kr\n",
    "import fasttext\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " def loadWordEmbedding(model):\n",
    "        '''\n",
    "            read pretrained \n",
    "        '''\n",
    "    model = fasttext.load_model(model)\n",
    "    vocab = ['unk']\n",
    "    embd = [[0] * 100]\n",
    "    for word in model.words:\n",
    "        vocab.append(word)\n",
    "        embd.append(model[word])\n",
    "    print(\"loaded word embedding\")\n",
    "    return vocab, embd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **tensorflow load the trained word embedding (fasttext)**<br >\n",
    " return the vocab and embedding<br>\n",
    " the embedding_dim is 100<br>\n",
    " the vocab: 226403\n",
    " - 'unk',\n",
    " - '处会',\n",
    " - '倍觉',\n",
    " - '螳螂捕蝉',\n",
    " - '烟标',\n",
    " - '见证人',\n",
    " - '玩到',\n",
    " - '刑满释放',\n",
    " \n",
    "the embedding:\n",
    "- [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
    "     0.        ,  0.        ],\n",
    "- [ 0.20675033,  0.13941619, -0.02178967, ...,  0.08394553,\n",
    "    -0.57525939, -0.17493315],\n",
    "- [ 0.02645796, -0.16807704, -0.33025488, ...,  0.18632847,\n",
    "    0.20377228,  0.1036511 ],\n",
    "\n",
    "note: add the unk is that the word not in trained word embedding. and the vector is 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train data format**\n",
    "- label /t document <br>\n",
    "```\n",
    "IT      互联网巨头雅虎公司26日宣布,公司正对高级管理层实施“大手术”,同时组建3个业务部门形成全新的公司组织构架。雅虎总裁苏珊·德克说,微软公司收购雅虎的谈判两周前结束后,雅虎终于可以专注于已耽搁数月的重组计划,为公司向赚钱机器转型迈出一大步。全新构架雅虎当天在声明中说,公司正在组建3个新业务部门,旨在把产品研发部门设为核心业务部门,同时加强产品研发部门与技术...\n",
    "```\n",
    "\n",
    "**train data cut format**\n",
    "- words[0] space words[1]..... space words[end] <br>\n",
    "```\n",
    "来源 : 新华网 加拿大 多伦多市 一名 建筑师 推出 的 摩天 农场 概念 引起 世人 关注 。 有别于 传统 农场 , 摩天 农场 向 空中 延伸 , 形似 摩天大楼 。 如果 摩天 农场 落成 , 农作物 产出 每年 可 满足 约 3.5 万名 居民 的 需求 。 目前 , 世界 上 已 出现 数个 类似 的 ... \n",
    "```\n",
    "\n",
    "- label number: 13\n",
    "- detail of train_data:\n",
    "```\n",
    " {'IT': 7021,\n",
    "  '体育': 58025,\n",
    "  '健康': 3486,\n",
    "  '军事': 1906,\n",
    "  '奥运': 18205,\n",
    "  '女性': 11295,\n",
    "  '娱乐': 22083,\n",
    "  '房产': 37069,\n",
    "  '教育': 6589,\n",
    "  '文化': 2106,\n",
    "  '旅游': 5945,\n",
    "  '汽车': 4180,\n",
    "  '财经': 40000}\n",
    "```\n",
    "\n",
    "note: **compute the average value of words length (seq_length)**\n",
    "\n",
    "**train data information**\n",
    "\n",
    "the words average is 317 for one news\n",
    "\n",
    "the longest words of news are 1717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnConfig(object):\n",
    "    def __init__(self, num_class, dim, vocab_size, seq_length=600, num_lay=2, hidden_dim=128, rnn='lstm', drop_keep_prob=0.5,\n",
    "                 learning_rate = 1e-3, batch_size=128, num_epochs = 50, print_per_batch=10, save_per_batch=10, embedding='one hot'):\n",
    "        self.embedding_dim = dim\n",
    "        self.seq_length = seq_length\n",
    "        self.num_classes = num_class\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.num_layer = num_lay\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = rnn\n",
    "\n",
    "        self.dropout_keep_prob = drop_keep_prob\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        self.print_per_batch = print_per_batch\n",
    "        self.save_per_batch = save_per_batch\n",
    "        \n",
    "        self.embedding = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRnn(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        \n",
    "        self.rnn()\n",
    "    \n",
    "    def rnn(self):\n",
    "        def lstm_cell():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(self.config.hidden_dim, state_is_tuple=True)\n",
    "        \n",
    "        def gru_cell():\n",
    "            return tf.contrib.rnn.GRUCell(self.config.hidden_dim)\n",
    "        \n",
    "        def dropout():\n",
    "            if self.config.rnn == 'lstm':\n",
    "                cell = lstm_cell()\n",
    "            else:\n",
    "                cell = gru_cell()\n",
    "            return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "        \n",
    "        with tf.device('/gpu:0'):\n",
    "            if self.config.embedding == 'embedding':\n",
    "                W = tf.Variable(tf.constant(0.0, shape=[self.config.vocab_size, self.config.embedding_dim]), trainable=False, name='W')\n",
    "                self.embedding_placeholder = tf.placeholder(tf.float32, [self.config.vocab_size, self.config.embedding_dim])\n",
    "                self.embedding_init = W.assign(self.embedding_placeholder)\n",
    "                embedding_inputs = tf.nn.embedding_lookup(W, self.input_x)\n",
    "                print('load the pretrained word vector')\n",
    "            else:\n",
    "                embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim])\n",
    "                embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "                print('load the one hot')\n",
    "\n",
    "        with tf.name_scope(\"rnn\"):\n",
    "            cells = [dropout() for _ in range(self.config.num_layer)]\n",
    "            rnn_cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "            _outputs, _ = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=embedding_inputs, dtype=tf.float32)\n",
    "            last = _outputs[:, -1, :]\n",
    "            \n",
    "        with tf.name_scope(\"score\"):\n",
    "            fc = tf.layers.dense(last, self.config.hidden_dim, name='fc1')\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "            \n",
    "            self.logits = tf.layers.dense(fc, self.config.num_classes, name='fc2')\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)\n",
    "            \n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "            \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correnct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correnct_pred, tf.float32))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_category(categories):\n",
    "    \"\"\"\n",
    "    categories transform to value \n",
    "    \"\"\"\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "    return categories, cat_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            yield (line.split('\\t')[1], line.split('\\t')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(content_dir, word_to_id, cat_to_id, max_length=1000, embedding='one hot'):\n",
    "    '''\n",
    "    make the train data\n",
    "    '''\n",
    "    contents = read_file(content_dir)\n",
    "    raw_data = []\n",
    "    labels = []\n",
    "    for content in contents:\n",
    "        raw_data.append(content[0])\n",
    "        labels.append(content[1])\n",
    "    \n",
    "    data_id, label_id = [], []\n",
    "    if embedding == 'one hot':\n",
    "        for i in range(len(raw_data)):\n",
    "            data_id.append([word_to_id[x] for x in raw_data[i] if x in word_to_id])\n",
    "            label_id.append(cat_to_id[labels[i]])\n",
    "            # print(data_id[len(data_id) - 1])\n",
    "            # print(label_id[len(data_id) - 1])\n",
    "        print('data processed!')\n",
    "    else:\n",
    "        data_id = [[word_to_id[x] if x in word_to_id else word_to_id['unk'] for x in raw.split(' ')] for raw in raw_data]\n",
    "        print('wordvector successfully')\n",
    "        label_id = [cat_to_id[label] for label in labels]\n",
    "        print('generated label ont hot')\n",
    "    \n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length, padding='post', truncating='post')\n",
    "    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))\n",
    "    \n",
    "    return x_pad, y_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "def get_time_dif(start_time):\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(x, y, batch_size=64):\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "    \n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indices]\n",
    "    \n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_data(x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob\n",
    "    }\n",
    "    \n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sess, x_, y_):\n",
    "    ## todo\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "    \n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "save_dir = './cnews/checkpoint/textRnn'\n",
    "train_dir = './cnews/cnews.train.txt'\n",
    "val_dir = './cnews/cnews.val.txt'\n",
    "save_path = './cnews//lstmModel/lstm.mod'\n",
    "\n",
    "\n",
    "def train(model, config):\n",
    "    print(\"Configuring TensorBoard and Saver...\")\n",
    "    tensorboard_dir = './cnews/TextRnn'\n",
    "    \n",
    "    tf.summary.scalar('loss', model.loss)\n",
    "    tf.summary.scalar('accuracy', model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    print('Loading training and validation data...')\n",
    "    start_time = time.time()\n",
    "    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    # x_train = x_train[:100]\n",
    "    # y_train = y_train[:100]\n",
    "    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    # x_val = x_val[:50]\n",
    "    # y_val = y_val[:50]\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    # print(x_train[0], y_train[0])\n",
    "    # print(x_val[0], y_val[0])\n",
    "    # print('time usage:', time_dif)\n",
    "    \n",
    "    gpu_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    session = tf.Session(config=gpu_config)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(session.graph)\n",
    "    if config.embedding == 'embedding':\n",
    "        session.run(model.embedding_init, feed_dict={model.embedding_placeholder: embedding})\n",
    "    \n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0\n",
    "    best_acc_val = 0.0\n",
    "    last_improved = 0\n",
    "    require_improvement = 10\n",
    "    \n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\n",
    "            # print(feed_dict)\n",
    "            \n",
    "            if  total_batch % config.save_per_batch == 0:\n",
    "                s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "                \n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                loss_val, acc_val = evaluate(session, x_val, y_val) #todo evaluate\n",
    "                \n",
    "                if acc_val > best_acc_val:\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "                \n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%}, ' + ' val Lpss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "                \n",
    "            session.run(model.optim, feed_dict=feed_dict)\n",
    "            total_batch += 1\n",
    "            \n",
    "            # if total_batch - last_improved > require_improvement:\n",
    "              #  print(\"No optimization for a long time, auto-stopping...\")\n",
    "              #  flag = True\n",
    "              #  break\n",
    "        # if flag:\n",
    "            # break    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train_test(data, file1, file2, size, categories):\n",
    "    train = []\n",
    "    test = []\n",
    "    for l in categories:\n",
    "        tmp_data = [(x, y) for x, y in data if x == l]\n",
    "        train_label, test_label = train_test_split(tmp_data, test_size=size)\n",
    "        train += train_label\n",
    "        test += test_label\n",
    "\n",
    "    write_to_file(file1, train)\n",
    "    write_to_file(file2, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(file, data):\n",
    "    with open(file, 'w', encoding='utf8') as f:\n",
    "        for x, y in data:\n",
    "            f.write(x + '\\t' + y)\n",
    "            f.flush()\n",
    "\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fasttextConfig(object):\n",
    "    def __init__(self, dim=100, ws=5, min_count=3):\n",
    "        self.dim = dim\n",
    "        self.ws = ws\n",
    "        self.min_count = min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab_dict(vocab, vocab_dir):\n",
    "    word_to_id = dict(zip(vocab, range(len(vocab))))\n",
    "    with open(vocab_dir, 'w', encoding='utf8') as f:\n",
    "        for key in word_to_id.keys():\n",
    "            f.write(key + '\\t' + str(word_to_id[key]) + '\\n')\n",
    "            f.flush()\n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def cut_sentence(doc, file):\n",
    "    data = read_file(doc)\n",
    "    f = open(file, 'w', encoding='utf8')\n",
    "    for x, y in data:\n",
    "        words = jieba.cut(x)\n",
    "        f.write(y + '\\t' + ' '.join(words))\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocab(vocab_dir):\n",
    "    with open(vocab_dir, 'r', encoding='utf8') as f:\n",
    "        words = [_.strip() for _ in f.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_model(doc, model, ft_config):\n",
    "    with open('./cnews/without.dat', 'w', encoding='utf8') as f:\n",
    "        data = read_file(doc)\n",
    "        for d, _ in data:\n",
    "            f.write(d)\n",
    "            f.flush()        \n",
    "    fasttext.skipgram('./cnews/without.dat', model,dim=ft_config.dim, ws=ft_config.ws, min_count=ft_config.min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dir, config, model_dir):\n",
    "    print(\"Loading test data...\")\n",
    "    start_time = time.time()\n",
    "    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    \n",
    "    gpu_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess = tf.Session(config=gpu_config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=sess, save_path=model_dir)\n",
    "    \n",
    "    print('Testing...')\n",
    "    loss_test, acc_test = evaluate(sess, x_test, y_test)\n",
    "    msg = 'Test Loss:{0:>6.2}, Test Acc:{1:>7.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "\n",
    "    batch_size = 128\n",
    "    data_len = len(x_test)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "    y_test_cls = np.argmax(y_test, 1)\n",
    "    y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        feed_dict ={\n",
    "            model.input_x: x_test[start_id : end_id],\n",
    "            model.keep_prob: 1.0\n",
    "        }\n",
    "\n",
    "        y_pred_cls[start_id:end_id] = sess.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "        \n",
    "    print('Precision, Recall and F1-Socre...')\n",
    "    print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n",
    "        \n",
    "    print('Confusion Matrix...')\n",
    "    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n",
    "    print(cm)\n",
    "        \n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, embd = loadWordEmbedding('model/fasttext.model.bin')\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = dict(zip(vocab, range(vocab_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **traisform to id**\n",
    "\n",
    "    **the y_label is like this:**\n",
    "        ```\n",
    "         {'IT': 0,\n",
    "          '体育': 1,\n",
    "          '健康': 2,\n",
    "          '军事': 3,\n",
    "          '奥运': 4,\n",
    "          '女性': 5,\n",
    "          '娱乐': 6,\n",
    "          '房产': 7,\n",
    "          '教育': 8,\n",
    "          '文化': 9,\n",
    "          '旅游': 10,\n",
    "          '汽车': 11,\n",
    "          '财经': 12})\n",
    "        ```\n",
    "\n",
    "    **the x_data is like this:**\n",
    "        ```\n",
    "        {'盖房子': 1,\n",
    "         '而书': 3,\n",
    "         '其他': 4,\n",
    "         '平房': 5,\n",
    "         '要放': 6,\n",
    "         '标本兼治': 7,\n",
    "         '失魂': 8,\n",
    "         '姚晓明': 9,\n",
    "         '银票': 10,\n",
    "         '斑秃': 11,\n",
    "         '份量': 12,\n",
    "         '四证': 13,\n",
    "         '小剂量': 15,\n",
    "        ```\n",
    "     \n",
    "note: plus the 'unk' in the vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**make the train data to index and pad in same length**\n",
    "- if the word vector size is smaller than specified length, then add the 0 (mode=post)\n",
    "- else delete the more word (mode=post)\n",
    "\n",
    "```\n",
    "([  5607, 215414,  15425,  19772,  52617, 111590,  19585,  40881,\n",
    "        86058, 121047, 147869, 204886, 216120, 154775, 207544, 218800,\n",
    "        81033, 130909, 167262,  30992, 139440,  19772,  80670, 193948,\n",
    "       135063,  31337, 213472, 215414, 198527,  40727, 178484,  38815,\n",
    "       187325, 177281,  83589, 178484,  38815,  39448, 210307, 180697,\n",
    "       177281, 216383, 116715,  42935,  85169, 207544, 218800,  44149,\n",
    "       197243, 177281, 130909, 207544, 218800, 154775, 160307, 111590,\n",
    "        81033,  43340,  70777,  12807,  15110, 177281, 123144,  63772,\n",
    "        81033, 178484, 174865,  70777, 137332, 160307, 111590, 213472,\n",
    "        71244, 177281,  19772,  52617, 100735,  81033,  56902, 211348,\n",
    "        ...,\n",
    "          0,      0,      0,      0,      0,      0,      0,      0,\n",
    "            0,      0,      0,      0,      0,      0,      0,      0], dtype=int32\n",
    "            \n",
    "```\n",
    "\n",
    "**make the category to index and transform to one-hot vector**\n",
    "\n",
    "```\n",
    "array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['体育' ,'财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "_, cat_to_id  = read_category(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.482 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.482 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "cut_sentence('./cnews/cnews.train.txt', './cnews/cnews_train_cut.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_sentence('./cnews/cnews.val.txt', './cnews/cnews_val_cut.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_config = fasttextConfig()\n",
    "fasttext_model('./cnews/cnews_train_cut.dat', './cnews/model/fasttext.mod', ft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded word2vec\n"
     ]
    }
   ],
   "source": [
    "vocab, embd = loadWordEmbedding('./cnews/model/fasttext.mod.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_id = read_vocab('./cnews/cnews.vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = generate_vocab_dict(vocab, './cnews/fasttext.vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_config = RnnConfig(len(categories), 64, len(words), rnn='gru',drop_keep_prob=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load the one hot\n",
      "WARNING:tensorflow:From <ipython-input-3-f3d943237286>:52: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = TextRnn(rnn_config)\n",
    "# train(model, rnn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load the one hot\n",
      "Loading test data...\n",
      "data processed!\n",
      "INFO:tensorflow:Restoring parameters from ./cnews/lstmModel/lstm.mod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./cnews/lstmModel/lstm.mod\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Test Loss:  0.22, Test Acc: 94.36%\n",
      "Precision, Recall and F1-Socre...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         体育       0.99      1.00      0.99      1000\n",
      "         财经       0.94      0.98      0.96      1000\n",
      "         房产       0.92      0.86      0.89      1000\n",
      "         家居       0.93      0.85      0.89      1000\n",
      "         教育       0.94      0.94      0.94      1000\n",
      "         科技       0.95      0.95      0.95      1000\n",
      "         时尚       0.93      0.97      0.95      1000\n",
      "         时政       0.90      0.96      0.93      1000\n",
      "         游戏       0.97      0.93      0.95      1000\n",
      "         娱乐       0.96      0.98      0.97      1000\n",
      "\n",
      "avg / total       0.94      0.94      0.94     10000\n",
      "\n",
      "Confusion Matrix...\n",
      "[[996   0   1   1   1   0   0   0   0   1]\n",
      " [  0 982   9   1   1   0   0   7   0   0]\n",
      " [  4  34 858  15  10   5   5  66   0   3]\n",
      " [  4   7  54 854  13  14  26  19   3   6]\n",
      " [  3   9   1   5 942  13   3  12  11   1]\n",
      " [  0   1   0  10   7 954  14   3  10   1]\n",
      " [  0   0   0  19   1   0 971   2   3   4]\n",
      " [  0   7   9   2   9  10   0 962   1   0]\n",
      " [  3   1   1   2  15   4  19   0 933  22]\n",
      " [  0   1   0   5   2   1   3   0   4 984]]\n",
      "Time usage: 0:00:49\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "rnn_config = RnnConfig(len(categories), 64, len(words), rnn='gru',drop_keep_prob=0.8)\n",
    "model = TextRnn(rnn_config)\n",
    "test('./cnews/cnews.test.txt', rnn_config, './cnews/lstmModel/lstm.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load the one hot\n",
      "Loading test data...\n",
      "data processed!\n",
      "INFO:tensorflow:Restoring parameters from ./cnews/lstmModel/lstm.mod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./cnews/lstmModel/lstm.mod\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Test Loss:   2.9, Test Acc: 47.60%\n",
      "Precision, Recall and F1-Socre...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         体育       0.98      0.64      0.78     24869\n",
      "         财经       0.80      0.36      0.49     17144\n",
      "         房产       0.52      0.34      0.41     15887\n",
      "         家居       0.00      0.00      0.00         0\n",
      "         教育       0.39      0.27      0.32      2825\n",
      "         科技       0.00      0.00      0.00         0\n",
      "         时尚       0.00      0.00      0.00         0\n",
      "         时政       0.00      0.00      0.00         0\n",
      "         游戏       0.00      0.00      0.00         0\n",
      "         娱乐       0.86      0.55      0.67      9465\n",
      "\n",
      "avg / total       0.79      0.48      0.59     70190\n",
      "\n",
      "Confusion Matrix...\n",
      "[[15911   472   556  4742   719   139   967   431   401   531]\n",
      " [   28  6105  3931  2511   160   317    26  3521   431   114]\n",
      " [   12   476  5444  8962   165    35   123   533    36   101]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   20   311   157  1043   767    78    10   232    97   110]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [  216   246   466  2335   168    82   419   214   134  5185]]\n",
      "Time usage: 0:05:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "rnn_config = RnnConfig(len(categories), 64, len(words), rnn='gru',drop_keep_prob=0.8)\n",
    "model = TextRnn(rnn_config)\n",
    "test('./cnews/Sougou_test.dat', rnn_config, './cnews/lstmModel/lstm.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one(documemt, model_dir, config):\n",
    "    gpu_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess = tf.Session(config=gpu_config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=sess, save_path=model_dir)\n",
    "    \n",
    "    data_id = [[word_to_id[x] for x in documemt if x in word_to_id]]\n",
    "    # print(data_id)\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, config.seq_length, padding='post', truncating='post')\n",
    "    \n",
    "    y_pred_cls = np.zeros(shape=1, dtype=np.int32)\n",
    "    feed_dict ={\n",
    "            model.input_x: x_pad,\n",
    "            model.keep_prob: 1.0\n",
    "        }\n",
    "    \n",
    "    y_pred_cls[0] = sess.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "    for category in cat_to_id.keys():\n",
    "        if cat_to_id[category] == y_pred_cls[0]:\n",
    "            print(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load the one hot\n",
      "INFO:tensorflow:Restoring parameters from ./cnews/lstmModel/lstm.mod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./cnews/lstmModel/lstm.mod\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "房产\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "rnn_config = RnnConfig(len(categories), 64, len(words), rnn='gru',drop_keep_prob=0.8)\n",
    "model = TextRnn(rnn_config)\n",
    "test_one('宏观财经要闻?央行:2008年前5个月金融市场总体运行平稳?国资委:严格控制职工持有国有大型企业股权?上海产权市场两大新“热点”市场动态?指数短期还会有反复周三沪深两市大盘小幅低开,先抑后扬。早盘,两市指数再创近期新低,沪综指和深成指分别下探至2729.71点、9114.19点,随后在可能会出利好的传闻推动下,两市展开强劲反弹行情,双双以中阳线报收,同时成交量较上一交易日放大约五成。终盘,沪综指报收2941.11点,大涨146.36点或5.24%,成交711.9亿;深成指报收9903.14点,大涨473.64点或5.02%,成交347.5亿。盘面上,今日两市个股呈现普涨态势,涨幅靠前主要是超跌股与低价股。热点方面,煤炭,有色金属,化工,能源,奥运,创投等概念均有较大反弹。其中以江山股份、浏阳花炮为首的化工股,以国阳新能为首的煤炭股,以锡业股份为首的有色金属股均涨幅居前,是主要做多动力,只有ST板块微跌。消息面上,值得关注的有:人民币加速升值。人民币对美元中间价17日升破6.9至6.8919,较前日上涨109基点,再创汇改以来新高。2008年以来,人民币对美元升值幅度接近6%;第四次中美战略经济对话17日在美国马里兰州安纳波利斯开幕。王岐山指出,深入探讨美国次贷危机及其影响,加强双方宏观经济和金融政策的协调,有利于维护两国以及世界经济金融稳定;能源和环境领域是中美经济合作新的增长点,双方合作空间广阔,应当努力取得更多成果。6月16日,中美企业界代表在美国首都华盛顿和密苏里州的圣路易斯市签署71项合同或协议,涉及大豆、节能机电产品、通信化工产品、飞机发动机、机械设备、通信及网络设备、半导体及电子器件等11大类产品,总金额约136亿美元。对于后市走势,我们认为如果没有实质性的利好出台,两市指数短期还会有反复,指数在恢复性上涨后可能会进一步下探,在经历一个震荡筑底阶段之后才会开始一波反弹行情。?新股定价行业公司?小商品城公告点评:新会展中心未来盈利或高于之前我们的预期,业绩的确定性、治理结构改善等提升投资价值,调高投资评级至“买入”?昆明机床子公司西安交大智能电器公司可能解散?中国航空工业集团公司筹备组已成立,中航一、二集团合并重组将正式拉开帷幕?香溢融通发布两则有关其典当业务风险的公告?步步高上市及一季度财务数据点评?中国平安5月份保费收入点评债券和衍生品?宝钢发规模100亿元可分离债,20日申购近期重点研究报告?金融工程:宝钢可分离债_债券报告?行业公司:中国人寿_跟踪报告?行业公司:银行业_深度报告?宏观研究:中国经济_宏观快报?宏观研究:美国经济_宏观快报新近推荐买入个股一览?S三九、天威视讯、煤气化、丽珠集团、凌钢股份、沈阳化工、雨润食品、天地科技、丹化科技、S*ST天颐搜狐证券声明:本频道资讯内容系转引自合作媒体及合作机构,不代表搜狐证券自身观点与立场,建议投资者对此资讯谨慎判断,据此入市,风险自担。', './cnews/lstmModel/lstm.mod', rnn_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
